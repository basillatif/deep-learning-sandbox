{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":96403,"databundleVersionId":11455419,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T02:50:49.492520Z","iopub.execute_input":"2025-04-09T02:50:49.492864Z","iopub.status.idle":"2025-04-09T02:51:03.948705Z","shell.execute_reply.started":"2025-04-09T02:50:49.492836Z","shell.execute_reply":"2025-04-09T02:51:03.947181Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import gradio as gr\nprint(gr.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T02:51:28.508052Z","iopub.execute_input":"2025-04-09T02:51:28.508482Z","iopub.status.idle":"2025-04-09T02:51:34.780055Z","shell.execute_reply.started":"2025-04-09T02:51:28.508449Z","shell.execute_reply":"2025-04-09T02:51:34.778813Z"}},"outputs":[{"name":"stdout","text":"5.24.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import gradio as gr\nfrom transformers import pipeline\n\n# Load the sentiment analysis model\nsentiment_pipeline = pipeline(\"sentiment-analysis\")\n\n# Prediction function\ndef classify_sentiment(text):\n    result = sentiment_pipeline(text)[0]\n    label = result['label']\n    score = result['score']\n    return f\"Prediction: {label} (Confidence: {score:.2f})\"\n\n# Create Gradio Interface\ninterface = gr.Interface(\n    fn=classify_sentiment,\n    inputs=gr.Textbox(lines=3, placeholder=\"Enter a tricky sentence to fool the model...\"),\n    outputs=\"text\",\n    title=\"Sentiment Classifier Challenge\",\n    description=\"Try to fool this Hugging Face sentiment model using sarcasm, ambiguity, or emotional tricks!\"\n)\n\n# Launch the app with public sharing\ninterface.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T21:42:46.460353Z","iopub.execute_input":"2025-04-08T21:42:46.461217Z","iopub.status.idle":"2025-04-08T21:43:20.538209Z","shell.execute_reply.started":"2025-04-08T21:42:46.461149Z","shell.execute_reply":"2025-04-08T21:43:20.537264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\nfrom transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nimport numpy as np\n\n# Load model + tokenizer\nmodel_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\nclassifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n\n# Attribution function\ndef interpret(text):\n    # Tokenize\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n    inputs.requires_grad_ = True\n\n    # Forward pass\n    outputs = model(**inputs)\n    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    pred_label = torch.argmax(probs, dim=-1).item()\n    confidence = probs[0][pred_label].item()\n\n    # Backward pass\n    outputs.logits[0][pred_label].backward()\n\n    # Get gradients\n    grads = inputs.input_ids.grad if hasattr(inputs.input_ids, \"grad\") else None\n    if grads is None:\n        grads = model.base_model.embeddings.word_embeddings.weight.grad\n    grads = inputs.input_ids.grad if inputs.input_ids.grad is not None else torch.zeros_like(inputs.input_ids)\n\n    # Get token importance via input gradients × input embeddings\n    input_embeds = model.base_model.embeddings.word_embeddings(inputs.input_ids)\n    grads = input_embeds.grad if input_embeds.grad is not None else torch.zeros_like(input_embeds)\n    importances = grads.abs().sum(dim=-1).squeeze().detach().numpy()\n\n    # Normalize importances\n    importances = importances / (importances.max() + 1e-9)\n\n    # Decode tokens and zip with scores\n    tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n    words = []\n    word_scores = []\n    for token, score in zip(tokens, importances):\n        if token.startswith(\"##\") and words:\n            words[-1] += token[2:]\n            word_scores[-1] = max(word_scores[-1], score)  # merge token scores\n        else:\n            words.append(token)\n            word_scores.append(score)\n\n    result = list(zip(words, word_scores))\n    return result, f\"{classifier(text)[0]['label']} (Confidence: {confidence:.2f})\"\n\n# Gradio app\ndemo = gr.Interface(\n    fn=interpret,\n    inputs=gr.Textbox(lines=3, placeholder=\"Try something sarcastic or tricky...\"),\n    outputs=[\n        gr.HighlightedText(label=\"Token Attribution\"),\n        gr.Text(label=\"Prediction\")\n    ],\n    title=\"🧠 Sentiment Classifier + Word Attribution\",\n    description=\"Try to trick the model, and see how much each word contributes to the prediction.\",\n)\n\ndemo.launch(share=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T22:20:20.849339Z","iopub.execute_input":"2025-04-08T22:20:20.849741Z","iopub.status.idle":"2025-04-08T22:20:24.910374Z","shell.execute_reply.started":"2025-04-08T22:20:20.849711Z","shell.execute_reply":"2025-04-08T22:20:24.909143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport gradio as gr\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch.nn.functional as F\n\n# Load model & tokenizer\nmodel_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\nmodel.eval()\n\nlabels = model.config.id2label  # {0: 'NEGATIVE', 1: 'POSITIVE'}\n\ndef interpret_and_score(text):\n    # Tokenize and get embeddings\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n    input_ids = inputs[\"input_ids\"]\n    attention_mask = inputs[\"attention_mask\"]\n\n    # Hook to store gradients\n    embeddings = model.distilbert.embeddings.word_embeddings(input_ids)\n    embeddings.retain_grad()\n\n    def forward_hook(module, input, output):\n        output.retain_grad()\n\n    handle = model.distilbert.embeddings.register_forward_hook(forward_hook)\n\n    outputs = model(inputs_embeds=embeddings, attention_mask=attention_mask)\n    logits = outputs.logits.squeeze()\n    probs = F.softmax(logits, dim=-1).detach().numpy()\n    pred_idx = torch.argmax(logits).item()\n    pred_label = labels[pred_idx]\n    confidence = probs[pred_idx]\n\n    # Backward pass\n    model.zero_grad()\n    logits[pred_idx].backward()\n\n    grads = embeddings.grad[0]  # shape: [seq_len, hidden_dim]\n    token_importance = grads.abs().sum(dim=1)  # shape: [seq_len]\n    token_importance = token_importance / (token_importance.max() + 1e-10)\n    token_importance = token_importance.detach().numpy()\n\n    # Convert tokens to words\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n    words = []\n    scores = []\n\n    for token, score in zip(tokens, token_importance):\n        if token.startswith(\"##\") and words:\n            words[-1] += token[2:]\n            scores[-1] = max(scores[-1], score)\n        else:\n            words.append(token)\n            scores.append(score)\n\n    highlighted = list(zip(words, scores))\n\n    # Detailed logit/prob table\n    class_scores = \"\\n\".join([\n        f\"{labels[i]}: Logit = {logits[i]:.3f}, Probability = {probs[i]*100:.2f}%\"\n        for i in range(len(labels))\n    ])\n\n    handle.remove()\n    return highlighted, f\"{pred_label} (Confidence: {confidence:.2f})\", class_scores\n\n# Gradio Interface\ndemo = gr.Interface(\n    fn=interpret_and_score,\n    inputs=gr.Textbox(lines=3, placeholder=\"Try something sarcastic...\"),\n    outputs=[\n        gr.HighlightedText(label=\"🧠 Word Attribution\"),\n        gr.Text(label=\"🎯 Prediction\"),\n        gr.Text(label=\"📊 Logits & Probabilities\")\n    ],\n    title=\"🧪 Sentiment Classifier + Word Impact + Class Scores\",\n    description=\"Enter a sentence. See prediction, word contributions, and detailed logits/probabilities.\",\n)\n\ndemo.launch(share=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T02:51:45.461167Z","iopub.execute_input":"2025-04-09T02:51:45.462316Z","iopub.status.idle":"2025-04-09T02:52:15.120248Z","shell.execute_reply.started":"2025-04-09T02:51:45.462243Z","shell.execute_reply":"2025-04-09T02:52:15.119049Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0736a12e15a1409c943deb1fb452c0b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d529f645c8f4c06a4b585dd8dd059dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2255e1f5af544a8f8a0563ba441a31d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcd22cb797e3440ca7aba1a89cbb8821"}},"metadata":{}},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://4ecdc4e209aa7ceb12.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://4ecdc4e209aa7ceb12.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":5}]}