{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The simplest example of a neural network to show in a deep learning class is a single-layer perceptron (SLP) or a feedforward neural network with the following properties:\n",
    "\n",
    "Example: Single-Layer Perceptron for Binary Classification\n",
    "Input: 2 features \n",
    "Output: A single binary classification (e.g., 0 or 1).\n",
    "Architecture:\n",
    "Input layer: 2 neurons (one for each feature).\n",
    "Output layer: 1 neuron with a step or sigmoid activation function.\n",
    "\n",
    "Decision boundary: A line (for step activation) separating the two classes.\n",
    "Teaching Notes:\n",
    "Start by explaining how the perceptron learns a linear decision boundary.\n",
    "Show training via a simple dataset like AND, OR, or XOR (explain why XOR fails without multiple layers).\n",
    "Demonstrate forward and backward propagation intuitively if possible.\n",
    "Tool/Framework:\n",
    "Use Python with a simple library like NumPy or TensorFlow/Keras for live coding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.4149\n",
      "Epoch [20/100], Loss: 0.2849\n",
      "Epoch [30/100], Loss: 0.2090\n",
      "Epoch [40/100], Loss: 0.1617\n",
      "Epoch [50/100], Loss: 0.1301\n",
      "Epoch [60/100], Loss: 0.1078\n",
      "Epoch [70/100], Loss: 0.0913\n",
      "Epoch [80/100], Loss: 0.0786\n",
      "Epoch [90/100], Loss: 0.0686\n",
      "Epoch [100/100], Loss: 0.0606\n",
      "\n",
      "Predictions:\n",
      "tensor([[5.4010e-04],\n",
      "        [6.4557e-02],\n",
      "        [6.8589e-02],\n",
      "        [9.0389e-01]])\n"
     ]
    }
   ],
   "source": [
    "# Data: AND logic\n",
    "X = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "y = torch.tensor([[0.0], [0.0], [0.0], [1.0]])\n",
    "\n",
    "# Define the model\n",
    "class SimplePerceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplePerceptron, self).__init__()\n",
    "        self.fc = nn.Linear(2, 1)  # 2 input features, 1 output neuron\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = SimplePerceptron()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(X)\n",
    "    print(\"\\nPredictions:\")\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
